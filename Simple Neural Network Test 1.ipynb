{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Data Preperation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a) Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "from scipy import spatial\n",
    "import operator, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1b) Importing and Preparing Dataset from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Scrapper/rating.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m rating_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Scrapper/rating.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Processign the rating text file\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#Slicing based on \":\" and accessing first and last index into dict\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(rating_file) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      8\u001b[0m     data \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m     data \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Scrapper/rating.txt'"
     ]
    }
   ],
   "source": [
    "# Adidng the rating to the pandas dataframe\n",
    "file_name = \"netflix_titles.csv\"\n",
    "rating_file = \"./Scrapper/rating.txt\"\n",
    "\n",
    "# Processign the rating text file\n",
    "#Slicing based on \":\" and accessing first and last index into dict\n",
    "with open(rating_file) as file:\n",
    "    data = file.read().split('\\n')\n",
    "    data = [i.split(':') for i in data]\n",
    "    data_dict = {}\n",
    "    for i in data:\n",
    "        if i[-1] not in ['NONE', '[ERROR]', '']:\n",
    "            data_dict['s'+i[0]] = float(i[-1]) \n",
    "\n",
    "# Opening the csv file as pandas data frame\n",
    "MovieData = pd.read_csv(file_name)\n",
    "\n",
    "# Dropping values if the ratings do not exist\n",
    "for index, val in enumerate(MovieData['show_id']):\n",
    "    if val not in data_dict:\n",
    "        MovieData = MovieData.drop(index)\n",
    "#Removed Drop '' from director because no such value\n",
    "\n",
    "\n",
    "# adding the movie/show ratungs to the dataFrame\n",
    "MovieData['rating'] = [i for i in data_dict.values()]\n",
    "\n",
    "#Preview\n",
    "MovieData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.scatterplot(data=MovieData, x = 'release_year', y = 'rating', hue='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.catplot(data=MovieData, x = 'type', y = 'rating', hue='rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the 2 graphs above, it is clear that ratings have no corelation with release year or type and hence they can be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1c) Cleanup of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the non-predictor columns as we will not need it for the prediction later\n",
    "MovieData.drop(columns=['show_id', 'description', 'country', 'duration', 'date_added', 'release_year', 'type'], inplace=True)\n",
    "\n",
    "# Removing the rows in which either directors, actors, or genres or both are not mentioned\n",
    "MovieData.dropna(inplace=True)\n",
    "\n",
    "# Converting strigns in cast and listed_in and cast columns to lists with strings\n",
    "MovieData['cast'] = MovieData['cast'].str.split(',')\n",
    "MovieData['listed_in'] = MovieData['listed_in'].str.split(',')\n",
    "\n",
    "# Resetting the index of the dataframe after removing the rows \n",
    "MovieData.reset_index(inplace=True)\n",
    "MovieData.drop(columns=['index'], inplace=True)\n",
    "\n",
    "#Preview\n",
    "MovieData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExploratoryData = MovieData.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Prep of Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a) Processing Genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizer for top Genre values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topVisualizer(series, Title, topN):\n",
    "    if(isinstance(series.iloc[0],list)):                        #Check if series values are lists\n",
    "        uniqelist = []                                          #Getting a Series of uniqe entries for 'listed_in' from list entries\n",
    "        for i in series:\n",
    "            uniqelist.extend(i)\n",
    "        toplist = pd.Series(uniqelist).value_counts()[:topN]    #Checking the most popular listed_in in column in ascending order\n",
    "    else: toplist = series.value_counts()[:topN]                \n",
    "    toplist.sort_values(ascending=True)\n",
    "\n",
    "    \n",
    "    plt.subplots(figsize=(12,10))               #Plot\n",
    "    plt.title(Title)\n",
    "    plot = toplist.plot.barh(width=0.9)         #Plot Graph\n",
    "    for i, value in enumerate(toplist.values):  #Labels\n",
    "        plot.text(.8, i, value,fontsize=12,color='white',weight='bold')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topVisualizer(ExploratoryData['listed_in'], 'Top Genres', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexer function for top Genre values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valuesindexer(series, topN=0):\n",
    "    isList = isinstance(series.iloc[0],list)    #Check if series values are lists\n",
    "    valueList = []\n",
    "    for index, values in series.items():\n",
    "        if(isList):\n",
    "            if(topN): values = values[:topN-1]  #Limit to only top N Entries\n",
    "            for value in values:            \n",
    "                if value not in valueList:\n",
    "                    valueList.append(value)     #Iterate trough all uniqe enties to build indexed list\n",
    "        else: \n",
    "            value = values\n",
    "            if value not in valueList:\n",
    "                valueList.append(value)         #Iterate trough all  enties to build indexed list\n",
    "\n",
    "    #Reference Series for Genre Indexer\n",
    "    IndexRef = {}\n",
    "    for i, value in enumerate(valueList): IndexRef[i] = value\n",
    "    IndexRef = pd.Series(IndexRef)\n",
    "    return valueList, IndexRef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valuesindexer(ExploratoryData[\"listed_in\"])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting Dataframe Column with binary list containing indexed entries for Genre values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarylistFormatter(check_list, list2):\n",
    "    binaryList = []                                                                    #Indexed (valuesindexer()[1] for keys) Binary-Formatted list of state if entry is present across columns\n",
    "    for value in list2:                                                                \n",
    "        if value in check_list:\n",
    "            binaryList.append(1)\n",
    "        else:\n",
    "            binaryList.append(0)\n",
    "    return binaryList\n",
    "\n",
    "def binarylistApplier(dataFrame, colReplaced, colName, topN=0):\n",
    "    valueslist = valuesindexer(dataFrame[colReplaced], topN)[0]                                #Get indexed list of uniqe values\n",
    "    dataFrame[colName] = dataFrame[colReplaced].apply(lambda x: binarylistFormatter(x, valueslist))   #Apply format as new column\n",
    "    dataFrame.drop(columns=colReplaced, inplace=True)                                          #Replace old column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarylistApplier(ExploratoryData, 'listed_in', 'Genre')\n",
    "ExploratoryData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b) Processing Cast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizer for top Vast values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topVisualizer(ExploratoryData['cast'], 'Top Actor', 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting Dataframe Column with binary list containing indexed entries for Cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarylistApplier(ExploratoryData, 'cast', 'Cast', 5)    # We only need to consider the actors that contributed the most in the movie, The dataset already contains the actors in order of their contribution\n",
    "ExploratoryData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows that do not have the direcots\n",
    "ExploratoryData[ExploratoryData['director']!='']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2c) Processing Director"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizer for top Director values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topVisualizer(ExploratoryData['director'], 'Top Director', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting Dataframe Column with binary list containing indexed entries for Dirctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarylistApplier(ExploratoryData, 'director', 'Director')\n",
    "ExploratoryData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) KNN Exploration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity(dataFrame, movieId1, movieId2):\n",
    "    a = dataFrame.iloc[movieId1]\n",
    "    b = dataFrame.iloc[movieId2]\n",
    "    \n",
    "    genresA = a['Genre']\n",
    "    genresB = b['Genre']\n",
    "\n",
    "    genreDistance = spatial.distance.cosine(genresA, genresB)\n",
    "    scoreA = a['Cast']\n",
    "    scoreB = b['Cast']\n",
    "    scoreDistance = spatial.distance.cosine(scoreA, scoreB)\n",
    "    \n",
    "    directA = a['Director']\n",
    "    directB = b['Director']\n",
    "    directDistance = spatial.distance.cosine(directA, directB)\n",
    "\n",
    "    return genreDistance + directDistance + scoreDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_score(dataFrame):\n",
    "    name = input('Enter a movie title: ').title()\n",
    "    new_movie = dataFrame[dataFrame['title'].str.contains(name)].iloc[0].to_frame().T\n",
    "\n",
    "    print('Selected Movie:', new_movie.title.values[0])\n",
    "\n",
    "    ind = dataFrame.index[dataFrame['title'] == name].tolist()[0]\n",
    "\n",
    "    def getNeighbors(baseMovie, K):\n",
    "        distances = []\n",
    "    \n",
    "        for index, movie in dataFrame.iterrows():\n",
    "            index = dataFrame.index[dataFrame['title'] == movie['title']].tolist()[0]\n",
    "            if movie['title'] != baseMovie['title'].values[0]:\n",
    "                dist = Similarity(dataFrame, index, ind)\n",
    "                distances.append((movie['title'], dist))\n",
    "    \n",
    "        distances.sort(key=operator.itemgetter(1))\n",
    "        neighbors = []\n",
    "    \n",
    "        for x in range(K):\n",
    "            neighbors.append(distances[x])\n",
    "        return neighbors\n",
    "    \n",
    "    K = int(math.sqrt(dataFrame.shape[0]))\n",
    "    avgRating = 0\n",
    "    neighbors = getNeighbors(new_movie, K)\n",
    "    \n",
    "    for neighbor in neighbors:\n",
    "        neighbor_index = dataFrame.index[dataFrame['title'] == neighbor[0]].tolist()[0]\n",
    "        avgRating = avgRating+dataFrame.iloc[neighbor_index]['rating']\n",
    "\n",
    "    avgRating = avgRating/K\n",
    "    print('The predicted rating for %s is: %f' %(new_movie['title'].values[0],avgRating))\n",
    "    print('The actual rating for %s is %f' %(new_movie['title'].values[0],new_movie['rating']))\n",
    "\n",
    "    error = (abs(float(avgRating)-float(new_movie['rating']))/float(new_movie['rating']))*100\n",
    "\n",
    "    print(\"calculated error : %0.2f\" %(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##predict_score(ExploratoryData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saved_file_name = 'Table_with_rating.csv'\n",
    "#df.to_csv(saved_file_name, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know that cast, director and genre can be used to predict the ratings quite accurately\n",
    "\n",
    "Now we will test different ML models to test which one is the best for our use case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestDF = MovieData.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the movie dataframe for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "labelEncoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# Converting the string in the director and title column to float -> Makes no difference to the computer\n",
    "TestDF['director'] = labelEncoder.fit_transform(TestDF['director'].values)\n",
    "\n",
    "TestDF['listed_in'] = TestDF['listed_in'].to_list()\n",
    "\n",
    "# Since the cast has way too many poeple, we are going to select the 4 with the most screen time\n",
    "TestDF['cast'] = TestDF['cast'].to_list()\n",
    "\n",
    "# Creatinng new columns for individual cast\n",
    "for i in range(4):\n",
    "    TestDF[f'cast_{i+1}'] = ''\n",
    "\n",
    "for ind, item in enumerate(TestDF['cast']):\n",
    "    for index, cast_a in enumerate(item[:4]):\n",
    "        TestDF[f'cast_{index+1}'][ind] = cast_a\n",
    "\n",
    "TestDF.drop(columns=['cast'], inplace=True)\n",
    "\n",
    "TestDF['cast_1'] = labelEncoder.fit_transform(TestDF['cast_1'].values)\n",
    "TestDF['cast_2'] = labelEncoder.fit_transform(TestDF['cast_2'].values)\n",
    "TestDF['cast_3'] = labelEncoder.fit_transform(TestDF['cast_3'].values)\n",
    "TestDF['cast_4'] = labelEncoder.fit_transform(TestDF['cast_4'].values)\n",
    "\n",
    "TestDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genre = []\n",
    "\n",
    "for genre_list in TestDF['listed_in']:\n",
    "    for genre in genre_list:\n",
    "        if genre not in unique_genre:\n",
    "            unique_genre.append(genre)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_no_listed_in = 0\n",
    "\n",
    "for ind, genre_list in enumerate(TestDF['listed_in']):\n",
    "    new_listed_in = []\n",
    "\n",
    "    for genre in genre_list:\n",
    "        new_listed_in.append(unique_genre.index(genre)+1)\n",
    "    max_no_listed_in = len(new_listed_in)\n",
    "    TestDF['listed_in'][ind] = new_listed_in\n",
    "\n",
    "\n",
    "for ind, genre_list in enumerate(TestDF['listed_in']):\n",
    "    while len(genre_list) < 3:\n",
    "        TestDF['listed_in'][ind].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TestDF.to_csv('ModifiedDF.csv')\n",
    "TestDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestDF = TestDF.drop('title', axis = 1)\n",
    "\n",
    "TestDF['listed_in'] = torch.FloatTensor(TestDF['listed_in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.is_tensor(TestDF['director'].iloc[0])\n",
    "#TestDF\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "NNtrain_set,NNtest_set = train_test_split(TestDF, test_size = 0.2)\n",
    "\n",
    "#NNtrain_set['listed_in'] = torch.FloatTensor(NNtrain_set['listed_in'])\n",
    "#NNtest_set['listed_in']  = torch.FloatTensor(NNtest_set['listed_in'])\n",
    "\n",
    "X_train = NNtrain_set.drop('rating', axis = 1)\n",
    "Y_train = NNtrain_set['rating']\n",
    "\n",
    "X_test = NNtest_set.drop('rating', axis = 1)\n",
    "Y_test = NNtest_set['rating']\n",
    "\n",
    "X = X_train.values\n",
    "Y = Y_train.values\n",
    "\n",
    "X = torch.FloatTensor(X)\n",
    "Y = torch.LongTensor(Y)\n",
    "\n",
    "#X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#create a neural network from the pytorch module, 1 input layer, 3 hidden layer, 1 output\n",
    "class Rate(nn.Module):\n",
    "    def __init__(self, predictor = 6, h1 = 36, h2 = 96, h3 = 192, h4=95, h5=80, h6 =20 , predict = 10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(predictor , h1)\n",
    "        self.fc2 = nn.Linear(h1, h2)\n",
    "        self.fc3 = nn.Linear(h2, h3)\n",
    "        self.fc4 = nn.Linear(h3, h4)\n",
    "        self.fc5 = nn.Linear(h4, h5)\n",
    "        self.fc6 = nn.Linear(h5, h6)\n",
    "        self.out = nn.Linear(h6, predict)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = self.out(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "rate = Rate()\n",
    "\n",
    "print(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#Choose Optimizer\n",
    "optimizer = torch.optim.Adam(rate.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 2500\n",
    "losses = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    #passing inputs through the network\n",
    "    y_pred = rate.forward(X)\n",
    "    \n",
    "     #measure the loss at each iteration\n",
    "    loss = criterion(y_pred,Y)\n",
    "   \n",
    "    #keep track of progress\n",
    "    losses.append(loss.detach().numpy())\n",
    "    \n",
    "    #print every selected interval\n",
    "    if i%10 == 0:\n",
    "        print(f'epoch: {i} and loss {loss}')\n",
    "    \n",
    "    #optimizing the model/feedback\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graphing the loss over iterations\n",
    "plt.plot(range(epoch),losses)\n",
    "plt.ylabel('loss/error')\n",
    "plt.xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.values\n",
    "Y_test = Y_test.values\n",
    "\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "Y_test = torch.LongTensor(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_eval = rate.forward(X_test)\n",
    "    loss = criterion(y_eval, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(X_test):\n",
    "        y_val = rate.forward(data)\n",
    "        \n",
    "        #print(f'{i+1}.) {str(y_val)}\\t {Y_test[i]}\\t{y_val.argmax().item()}')\n",
    "        print(f'{i+1}.) {Y_test[i]}\\t{y_val.argmax().item()}')\n",
    "        \n",
    "        if y_val.argmax().item() == Y_test[i]: \n",
    "            correct += 1\n",
    "\n",
    "            \n",
    "    print(f'We got {correct} correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
